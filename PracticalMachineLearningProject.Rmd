---
title: "Practical Machine Learning - Project"
date: "Tuesday, November 17, 2015"
output: html_document
---
###Load necessary Libraries
```{r LoadMyLibraries, echo=TRUE, message=FALSE}
library(randomForest); library(dplyr); library(caret)  
library(rattle); library(rpart.plot); library(scales)
```

###Background (From Assignment)
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).  

###Data (From Assignment)
The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.  

###Executive Summary  
Two models were built to predict the Activity Quality, a tree model and a random forest model.  The Tree model performed poorly with approximately 50% accuracy and was not cross validated with the testing dataset due to the poor performance on the training dataset.  The Random Forest model generated an accurate estimate with the training dataset having 100% accuracy and cross validating with the testing dataset having 99.6% accuracy and an out of sample error rate of 0.4%.  The Random Forest model was able to predict 20 out of 20 of the sample cases provided correctly.  

###Project Outline  
I will follow the steps to complete this project.  

 1:  Read in Datasets.  
 2:  Cleanse the Data.  
 3:  Segment the data into Training and Test groups.  
 4:  Create a Machine Learning Algorithm to predict Activity Quality (the classe value).  
 5:  Cross Validation of Final Model (including estimating the out of sample error).  
 6:  Predict the outcomes of the 20 test samples.  

###1: Reading in Data  
```{r ReadAssignmentFiles, echo=TRUE}
train <- read.csv('c:/coursera/Course8Lectures/pml-training.csv')
test  <- read.csv('c:/coursera/course8Lectures/pml-testing.csv')
```

###2: Cleanse the Data  
In the Appendix I used the str function to get a glimpse into the data.  It appears there are a significant number of columns with NAs and Blank cells.  I will first remove all columns with more than 90% NAs, then remove columns with more than 90% blank cells, then I will remove the first 6 columns which should have limited predictive value (Name of subject, Timestamps etc) and finally I will verify that there are no remaining columns with near zero variances.  

```{r CleanseData, echo=TRUE, cache=TRUE}
#Identify columns with less than 90% NAs and retain them
TooManyNAs <- apply(train, 2, function(x) sum(is.na(x))/nrow(train) < 0.9)
train2 = train[,TooManyNAs]

#Identify columns with less than 90% Blanks and retain them
TooManyNulls <- apply(train2, 2, function(x) sum(x=="")/nrow(train2) < 0.9)
train3 = train2[,TooManyNulls]

#Remove 1st six columns
train4 <- train3[,7:length(train3)]

#Check Remaining columns for low variance columns
LowVarCols <- nearZeroVar(train4, freqCut = 95/5)
NumberOfColumns <- length(train4) - 1
```

Note:  There are `r length(LowVarCols)` columns remaining with low variances, so I will use this as my final dataset. My final dataset has `r NumberOfColumns` columns in addition to the variable (classe) we are predicting.   

###3: Create Training and Test Groups  
I will split my dataset 60/40 Training/Test and I will set my seed to 1234 prior to creating the partitions.  
```{r TrainingAndTestGroups, echo=TRUE}
set.seed(1234)
inTrain <- createDataPartition(y=train4$classe,
                               p=0.60, list=FALSE)
training <- train4[inTrain,]
testing <- train4[-inTrain,]
```

###4:  Create a Machine Learning Algorithm to predict Activity Quality.  
####Tree Model  
I will use caret's rpart method to build a tree on my training dataset.  

```{r CreateTree, echo=TRUE, cache=TRUE}
ProdModFit1 <- train(classe ~ .,method="rpart",data=training)
Mod1Predict <- predict(ProdModFit1, training)
Mod1Matrix <- confusionMatrix(training$classe,Mod1Predict)
Mod1Accuracy <- Mod1Matrix$overall[1]

Mod1Matrix$overall

Mod1Matrix$table
```

Looking at the results of my tree, the accuracy of the prediction is only `r percent(Mod1Accuracy)`.  My confusion matrix above also shows that my tree isn't a very good predictor.  I've included a visual representation of my tree in my Appendix.

####Random Forecst Model
Because my tree is not a very good predictor, I will attempt to build a random forecast model to see if I can get better accuracy with my training dataset.  Before building the tree, I will reset my seed to 1234.  

```{r CreateForest, echo=TRUE, cache=TRUE}
set.seed(1234)
ProdModFit2 <- randomForest(classe~.,data=training)
Mod2Predict <- predict(ProdModFit2, training)
Mod2Matrix <- confusionMatrix(training$classe,Mod2Predict)
Mod2Accuracy <- Mod2Matrix$overall[1]

Mod2Matrix$overall

Mod2Matrix$table
```

Looking at the results of my Random Forest, the accuracy of the prediction is `r percent(Mod2Accuracy)`, so this appears to be a pretty good model.  My confusion matrix above also shows that my Random Forest is a very good predictor as well.  

###5:  Cross Validation of Final Model (estimate out of sample error).  
I will use the Random Forest model built above to predict the outcomes on my test data and Cross Validate those outcomes with the actual outcomes in the test dataset.  Based on the training dataset, I would expect my out of sample error rate to be low, but probably above the 0% error rate that I saw in my training dataset.  


```{r CrossValidate, echo=TRUE, cache=TRUE}
Mod2PredictTest <- predict(ProdModFit2, testing)
Mod2MatrixTest <- confusionMatrix(testing$classe,Mod2PredictTest)
Mod2AccuracyTest <- Mod2MatrixTest$overal[1]

Mod2MatrixTest$overall

Mod2MatrixTest$table
```

The model Cross Validated pretty well with accuracy of `r percent(Mod2AccuracyTest)`.  My estimate for the out of sample error rate is then: `r percent(1 - Mod2AccuracyTest)` or 1 - `r percent(Mod2AccuracyTest)`

###6:  Predict the outcomes of the 20 test cases.  
I will use my Random Forest model to predict the 20 test case outcomes.  

```{r TestPredict, echo=TRUE, cache=TRUE}
TestPredict <- predict(ProdModFit2, test)  
TestPredict
```

###Conclusion  
The Random Forest model did a good job of predicting the correct outcome.  My out of sample error rate was 0.4% and I was able to correctly predict 20 out of 20 of the test cases.  

###Appendix
####Exploration of dataset  
```{r ExploreData, echo=TRUE}
str(train)
```

####Visual Representation of my Tree
```{r Model1Tree, echo=TRUE}
fancyRpartPlot(ProdModFit1$finalModel)
```
